import os
from dotenv import load_dotenv
import nltk

# ======================
# 1. Chu·∫©n b·ªã NLTK
# ======================
nltk.download("punkt")
try:
    nltk.download("punkt_tab")  # c·∫ßn cho NLTK >=3.8.1
except:
    pass

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import NLTKTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

from langchain.schema import Document

# Load PDF
loader = PyPDFLoader("marketing.pdf")
pages = loader.load()

# G·ªôp to√†n b·ªô text th√†nh 1 Document duy nh·∫•t
full_text = "\n".join([p.page_content for p in pages])
full_doc = [Document(page_content=full_text)]

# ======================
# 3. Chia nh·ªè vƒÉn b·∫£n b·∫±ng NLTK
# ======================
text_splitter= NLTKTextSplitter(
    chunk_size=1600,       # ƒë·ªô d√†i t·ªëi ƒëa m·ªói chunk (s·ªë k√Ω t·ª±)
    chunk_overlap=400,     # s·ªë k√Ω t·ª± overlap gi·ªØa 2 chunk
    separator="\n\n"       # k√Ω t·ª± t√°ch ƒëo·∫°n (m·∫∑c ƒë·ªãnh theo NLTK sentence tokenizer)
)
splitted_docs = []

for doc in full_doc:
    chunks = text_splitter.split_text(doc.page_content)
    for chunk in chunks:
        splitted_docs.append(
            {
                "page_content": chunk,
                "metadata": doc.metadata,  # gi·ªØ metadata (trang s·ªë, v.v.)
            }
        )

print(f"‚úÇÔ∏è Sau khi chia chunk: {len(splitted_docs)} ƒëo·∫°n")

# ======================
# 4. Kh·ªüi t·∫°o Embeddings
# ======================
device = "cuda" if os.environ.get("USE_GPU", "1") == "1" else "cpu"
embeddings = HuggingFaceEmbeddings(
    model_name="intfloat/multilingual-e5-large-instruct",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True},
)

# ======================
# 5. T·∫°o vector store t·ª´ text
# ======================
vectorstore = FAISS.from_texts(
    [d["page_content"] for d in splitted_docs],
    embeddings,
    metadatas=[d["metadata"] for d in splitted_docs],
)

# Save to disk (create folder if not exists)
save_path = "vector_db2chunk_nltk"
os.makedirs(save_path, exist_ok=True)
vectorstore.save_local(save_path)



# # ======================
# # 6. Truy v·∫•n th·ª≠
# # ======================
# query = "ƒê·∫∑t t√™n trong java"
# retriever = vectorstore.as_retriever()
# results = retriever.get_relevant_documents(query)
#
# print("üîç K·∫øt qu·∫£ truy v·∫•n:")
# for i, d in enumerate(results, 1):
#     print(f"\n--- K·∫øt qu·∫£ {i} (Trang {d.metadata.get('page', 'N/A')}) ---")
#     print(d.page_content)
